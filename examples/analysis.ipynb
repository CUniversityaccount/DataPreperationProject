{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(model, text1, text2):\n",
    "    doc1 = model(text1)\n",
    "    doc2 = model(text2)\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate emotional similarity. return 1 if the two texts have the same emotion, otherwise return 0\n",
    "def get_emotional_similarity(model, text1,text2):\n",
    "    text1 = text1[:512]\n",
    "    text2 = text2[:512]\n",
    "\n",
    "    result1 = model(text1)\n",
    "    result2 = model(text2)\n",
    "    if result1[0]['label'] == result2[0]['label']:\n",
    "        return 1 #consistent\n",
    "    else:\n",
    "        return 0 #inconsistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the cosine similarity between two texts\n",
    "def get_cosine_similarity(tokenizer, model, text1, text2):    \n",
    "    inputs1 = tokenizer(text1, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    inputs2 = tokenizer(text2, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "    \n",
    "    embeddings1 = outputs1.last_hidden_state.mean(dim=1)\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "    similarity = cosine_similarity(embeddings1[0].cpu(), embeddings2[0].cpu()).item()\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the logical relationship between two texts\n",
    "def get_logical_relation(tokenizer, model, text1, text2):\n",
    "    text1 = tokenizer.convert_tokens_to_string(tokenizer.tokenize(text1)[:509])\n",
    "    text2 = tokenizer.convert_tokens_to_string(tokenizer.tokenize(text2)[:509])\n",
    "    result = model(f\"{text1} [SEP] {text2}\")\n",
    "    return result[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(df1, df2, word_vector_similarity, sentiment_pipeline, tokenizer_nli, nli_model, tokenizer_bert, bert_model, device):\n",
    "    similarity_sum = 0\n",
    "    emotional_similarity_sum = 0\n",
    "    cosine_similarity_sum = 0\n",
    "    logical_relation_sum = 0\n",
    "    count = 0\n",
    "    for text1, text2 in zip(df1['reviewText'], df2['reviewText']):\n",
    "        text1 = str(text1)\n",
    "        text2 = str(text2)\n",
    "        similarity = get_similarity(word_vector_similarity, text1, text2)\n",
    "        emotional_similarity = get_emotional_similarity(sentiment_pipeline, text1, text2)\n",
    "        cosine_similarity = get_cosine_similarity(tokenizer_bert, bert_model, text1, text2)\n",
    "        logical_relation_label = get_logical_relation(tokenizer_nli, nli_model, text1, text2)\n",
    "\n",
    "        similarity_sum += similarity\n",
    "        emotional_similarity_sum += emotional_similarity\n",
    "        cosine_similarity_sum += cosine_similarity\n",
    "        if logical_relation_label == 'entailment':\n",
    "            logical_relation_sum += 1\n",
    "        count += 1\n",
    "\n",
    "    average_similarity = similarity_sum / count\n",
    "    average_emotional_similarity = emotional_similarity_sum / count\n",
    "    average_cosine_similarity = cosine_similarity_sum / count\n",
    "    average_logical_relation = logical_relation_sum / count\n",
    "    # print('count', count)\n",
    "    \n",
    "    return average_similarity, average_emotional_similarity, average_cosine_similarity, average_logical_relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "word_vector_similarity= spacy.load('en_core_web_md') \n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model='distilbert/distilbert-base-uncased-finetuned-sst-2-english', device=device)\n",
    "\n",
    "tokenizer_nli = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "nli_model = pipeline('text-classification', model='facebook/bart-large-mnli', device=device)\n",
    "\n",
    "df1 = pd.read_json('./data/test.json',lines=True)\n",
    "df2_name = ['ta_clean_psc_test', 'ta_clean_tb_test', 'dd_clean_psc_test', 'dd_clean_tb_test', 'jg_clean_psc_test', 'jg_clean_tb_test']\n",
    "\n",
    "for name in df2_name:\n",
    "    df2 = pd.read_json(f'./data/{name}.json', lines=True)\n",
    "    average_similarity, average_emotional_similarity, average_cosine_similarity, average_logical_relation = get_similarity_score(df1, df2, word_vector_similarity, sentiment_pipeline, tokenizer_nli, nli_model, tokenizer_bert, bert_model, device)\n",
    "    # average_similarity, average_emotional_similarity, average_cosine_similarity = get_similarity_score(df1, df2, word_vector_similarity, sentiment_pipeline, tokenizer_nli, nli_model, tokenizer_bert, bert_model, device)\n",
    "\n",
    "    print('')\n",
    "    print('name: ', name)\n",
    "    print('average_similarity: ', average_similarity)\n",
    "    print('average_emotional_similarity: ', average_emotional_similarity)\n",
    "    print('average_cosine_similarity: ', average_cosine_similarity)\n",
    "    print('average_logical_relation: ', average_logical_relation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
