{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from cleaning.lstm import LSTMAutoencoder, TextDataset, find_spelling_errors, train_lstm\n",
    "from cleaning.bert_lora import LoRABert, fine_tune_model, BertModel, CustomDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_word(df):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "    model = torch.load('ckpt/lstm_model.pth').to(device)\n",
    "\n",
    "    misspelled_words = find_spelling_errors(df, tokenizer, model, device)\n",
    "\n",
    "    misspelled_words = []\n",
    "    for index, word in misspelled_words:\n",
    "        misspelled_words.append((index, word))\n",
    "\n",
    "    return misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_spelling_errors(df, misspelled_words):\n",
    "    for index, word in misspelled_words:\n",
    "        text = df.at[index, 'reviewText']\n",
    "        corrected_text = text.replace(word, '[MASK]')\n",
    "        df.at[index, 'reviewText'] = corrected_text\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_correct_spelling(df):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "    bert_model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    lora_model = LoRABert.from_pretrained('ckpt/lora_model.pth')\n",
    "    lora_model.to(device)\n",
    "    lora_model.eval()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        review_text = row['reviewText']\n",
    "        inputs = tokenizer(\n",
    "            review_text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ) \n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            completed_text = lora_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        df.at[index, 'reviewText'] = completed_text\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_json(\"data/corrupted_test.json\",\n",
    "            #   compression=\"gzip\", \n",
    "                lines=True)\n",
    "df_2 = pd.read_json(\"data/test.json\",\n",
    "            #   compression=\"gzip\", \n",
    "                lines=True)\n",
    "\n",
    "train_lstm(df_1)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "\n",
    "num_classes = 2\n",
    "lora_model = LoRABert(model, num_classes)\n",
    "\n",
    "texts = df_1['reviewText'].tolist()\n",
    "labels = df_2['label'].tolist()\n",
    "max_length = 512\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(lora_model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "fine_tune_model(lora_model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "\n",
    "for index, row in df_1.iterrows():\n",
    "    review_text = row['reviewText']\n",
    "\n",
    "    inputs = tokenizer(review_text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        completed_text = lora_model(input_ids, attention_mask)\n",
    "    df_1.at[index, 'reviewText'] = completed_text\n",
    "\n",
    "df = mlm_correct_spelling(df_1)\n",
    "\n",
    "pd.to_json(df, \"data/clean_psc_test.json\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
